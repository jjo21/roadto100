---
layout: post
title: Decision Trees, Math and Optimizations 
subtitle: The math and optimizations for XGBoost
tags: [Machine Learning, Data Science, Python, Decision Trees, XGBoost]
comments: true
js:
    - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML
---

This page serves more as my own personal notes rather than anything else. For more in depth explanation and formal 
mathematics, please check out other sources. 

In general, for any Machine Learning algorithm, a loss function must be minimised to obtain the parameters of the model 
which can be used to make predictions for regression or classification. 

For regression the loss function is given by: $$ L(y_i,p_i) = \frac{1}{2}(y_i - p_i)^2 $$ where $$y_i$$ is the
actual value and $$p_i$$ is the predicted value. For classification the loss function is given by $$ L(y_i,p_i) = -[y_i log(p_i) + (1 - y_i)log(1 - p_i)] $$. 

A second order Taylor approximation is performed on the loss function and the derivative with respect to the output value 
is taken in order to find the output value which minimizes the loss function.

The formula of the output value and cover is given by the solution to the minimisation of the second order Taylor 
polynomial approximation of the loss function. The formula for the similarity score is also found from the formula for 
the output value. The similarity in the formula is what gives XGBoost its characteristic extreme gradient boosting 
because it serves to reduce the complexity of the algorithm by a large amount.

![image]({{ '/assets/img/Data_Folder/2023-09-22-dt(math)/photo3.jpg' | relative_url }})
![image]({{ '/assets/img/Data_Folder/2023-09-22-dt(math)/photo2.jpg' | relative_url }})
![image]({{ '/assets/img/Data_Folder/2023-09-22-dt(math)/photo1.jpg' | relative_url }})

{: .box-note}
**Greedy Algorithm:** XGBoost looks at every gain for every threshold. Using the largest threshold with the largest gain 
makes XGBoost greedy because it makes a decision without looking ahead to see if it is the absolute best in the long term. 
By using a greedy algorithm XGBoost can build a tree quickly. With a lot of measurements, XGBoost becomes slow. That's why
an approximate Greedy Algorithm is used where data is divided into quantiles these quantiles are used as thresholds
to calculate gain.



