---
layout: post
title: Bayes Theorem
subtitle: An explanation and demonstration
tags: [test, statistics, probability-distributions, visualisation]
comments: true
js:
    - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML
---

<div style="text-align: justify">
Bayes' Theorem is a tool you can use for updating your beliefs or predictions as you gather 
new information. It helps you make better decisions in uncertain situations.

Bayes' Theorem is a fundamental concept in probability theory and statistics. It's named after Thomas Bayes, an 
18th-century mathematician and philosopher. The theorem relates conditional probabilities, which means the probability 
of something happening given that something else has already happened.
</div>

Here's the formula:

$$
P(H \mid E) = \frac{P(H)P(E \mid H)}{P(E)}
$$

where:
* $$P(H \mid E)$$ is the probability of hypothesis $$H$$ given the evidence $$E$$ has occurred. This is called the posterior.
* $$P(H)$$ is the probability of hypothesis $$H$$ being true (regardless of the evidence $$E$$). This is called the prior.
* $$P(E \mid H)$$ is the probability of the evidence $$E$$ given that hypothesis $$H$$ is true. This is called the likelihood.
* $$P(E)$$ is the probability of the evidence $$E$$ occurring (regardless of the hypothesis $$H$$). This is called the marginal.

<div style="text-align: justify">
To understand the formula better, we can use the following example. Suppose we work in a company and know that 80% of 
the people belong in the data science department and 20% in the marketing department (very unlikely for a real company). 
__What is the probability that a new colleague you just observed in the break room is from marketing?__ 
From the frequentist view, we would say that the probability is 20%. However, we can also use Bayes' theorem to 
calculate the probability given the evidence that our new colleague is extroverted.
</div>

![image]({{ '/assets/img/Data_Folder/2023-09-18-bayes/bayes1.png' | relative_url }})

<div style="text-align: justify">
Given some extra information, we are able to represent the entire "space" of all probabilities as a square and divide 
up the people in the company based on their job and personality. $$P(E \mid H1)$$ is the probability of observing an 
extroverted person given the hypothesis that the person is from the marketing department. $$P(E \mid H2)$$ is the 
probability of observing an extroverted person given the hypothesis that the person is from the data science department.
</div>

![image]({{ '/assets/img/Data_Folder/2023-09-18-bayes/bayes2.png' | relative_url }})

Using Bayes Theorem, the probability that our new colleague has risen to 50%. To see more examples and to experiment with
Bayes Theorem yourself, I have written up some code in Python to help you implement it. The code is available on my
[GitHub](https://github.com/jjo21/Bayes/blob/main/bayes.py). Inspiration and examples are taken from Think Bayes by Allen B.
Downey.

Example (Cookie Problem)
--------------------------
<div style="text-align: justify">
You have 2 bowls, each with vanilla and chocolate cookies. You pick a bowl at random and then pick 5 vanilla cookies 
with replacement at random. What is the probability that it came from Bowl 1? 
The distribution of vanilla and chocolate cookies in each bowl are:
</div>

$$Bowl1 = \{Vanilla: 30, Chocolate: 10\}$$
$$Bowl2 = \{Vanilla: 20, Chocolate: 20\}$$

```python
from bayes import Bayes
Cookie = Bayes()
Cookie.set('Bowl 1', 0.5)
Cookie.set('Bowl 2', 0.5)
mix1 = dict(van=0.75, choc=0.25)
mix2 = dict(van=0.5, choc=0.5)
Cookie.mix('Bowl 1', mix1)
Cookie.mix('Bowl 2', mix2)

data1 = dict(van=5, choc=0)
Cookie.update(data1)
Cookie.likelihood()
plt.hlines(0.5, -3, 3, colors='r', label='Prior')
plt.bar(Cookie.prior.keys(),Cookie.posterior().values(), label='Posterior')
plt.legend()
plt.show()
```

![image]({{ '/assets/img/Data_Folder/2023-09-18-bayes/cookie.png' | relative_url }})
<div style="text-align: justify">
When initialising a prior, we make the guess of 50% for each bowl and systematically update our beliefs with the 
probability given to us by the distribution of cookies in each bowl. We can see that the probability of the cookie coming
from Bowl 1 is now more than 80%, which makes sense because there are more vanilla cookies in Bowl 1 than in Bowl 2.
</div>

Example 2 (Multiple Bowls)
--------------------------
<div style="text-align: justify">
We now have 100 bowls, each with a different number of vanilla cookies proportionate to the bowl number. Bowl 1 has 1
vanilla cookie, Bowl 2 has 2 vanilla cookies, Bowl 3 has 3 vanilla cookies and so on. We pick a bowl at random and then 
draw cookies as shown below. Before drawing from any bowl, the probability of each bowl is the same. 
</div>

```python
from bayes import Bayes
Cookie2 = Bayes()
for i in range(1, 101):
    Cookie2.set('Bowl ' + str(i), 1/100)

# plot the prior
x = np.arange(1, 101)
y = Cookie2.prior.values()

for i in range(1,101):
    mix = dict(van=i/100, choc=(1-i/100))
    Cookie2.mix('Bowl ' + str(i), mix)

data2 = dict(van=2, choc=1)
Cookie2.update(data2)
y1 = Cookie2.posterior()

plt.plot(x, y, label='Prior')
plt.plot(x, y1.values(), label='Posterior')
plt.xlabel('Bowl Number')
plt.legend()
plt.show()
print(Cookie2.get_max())

cdf = Cookie2.credible_interval()
```

<div style="text-align: justify">
With 1 vanilla cookie being drawn, we observe a linear posterior. The bowl with the highest probability is bowl 100 with
all vanilla cookies.
</div>  

![image]({{ '/assets/img/Data_Folder/2023-09-18-bayes/cookie2.png' | relative_url }})

<div style="text-align: justify">
With 2 vanilla and 1 chocolate cookies being drawn we observe the following posterior. The bowl with the highest probability
is bowl 67. Intuitively this makes sense because the number of vanilla cookies in that bowl is 67% of the total number of
cookies.
</div>

![image]({{ '/assets/img/Data_Folder/2023-09-18-bayes/cookie3.png' | relative_url }})

I hope this post has helped you understand Bayes Theorem a bit better.
