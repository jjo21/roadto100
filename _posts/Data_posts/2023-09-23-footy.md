---
layout: post
title: An example of XGBoost on Premier League Data
subtitle: Feature selection, data analysis, hyperparameter tuning and model evaluation
tags: [Machine Learning, Data Science, Python, Decision Trees, Football, Premier League]
comments: true
js:
    - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML
---

In this post I will be using XGBoost to predict the outcome of a Premier League football club. Some constraints on the data
used include: trying to predict and classify the outcome of premier league games. The data is taken from 
[here](https://www.football-data.co.uk/).

The main features used include the betting odds from various bookmakers, the home and away team and the referee of the game.
Match statistics are not used and columns with match statistics are omitted from the training and testing data. Initial 
data cleaning performed on the data set includes one hot encoding to convert categorical data i.e. (Teams) 
into numerical data. The composition of teams making up the Premier League have also changed across the years, the spending
talent and management of many clubs have also changed across the years. These values are hard to capture and quantify, 
therefore I am hoping that using betting odds to predict the outcome of a game will be a good proxy for the quality of
a team. 

There will be different data and feature selection for the model to capture its performance. Since the feature and data
selection for each model is different, they cannot be compared directly. The purposes of this post is then to show the 
process of feature selection, data analysis, hyperparameter tuning and model evaluation.

Data Selection 1
--------------------------
__Features__:  
- Teams which have not been relegated for the past 9 seasons (Excluding the 23/24 season which will be used for
the final test) These teams include: {'Arsenal', 'Chelsea', 'Crystal Palace', 'Everton', 'Liverpool', 'Man City', 'Man United', 
'Tottenham', 'West Ham'}  
- Betting Data   
- Home & Away Data

Hyperparmeter Selection: Automatic (Using the Classic XGBoost Model for training) 

Using data for the past 9 seasons, we arrive observe the following confusion matrix.
![image]({{ '/assets/img/Data_Folder/2023-09-23-footy/result1.png' | relative_url }})

Results show that the model is better in predicting the outcome of a home win and away win. The model is not as good at
predicting the results of a draw. In order to improve the model, we could further improve the hyperparameter choice 
through either GridSearchCV or RandomSearchCV. 

Additionally, there is only a total of 728 rows of data. The amount of tabular data is insufficient given the number of 
classes the algorithm has to predict. 10 seasons of data could also be too long a time selection, as many clubs would 
have undergone financial and management evolutions. 

|          | Precision | Recall   | F1       |
|:---------|:----------|:---------|----------|
| Away Win | 0.450980  | 0.489362 | 0.469388 |
| Draw     | 0.188679  | 0.285714 | 0.227273 |
| Home Win | 0.697368  | 0.540816 | 0.609195 |

Precision: TP / (TP + FP) Quality of Positive Results  
Recall: TP / (TP + FN) Percentage of Actual Positives correctly classified  
Specificity: TN / (TN + FP): Percentage of Actual Negatives correctly classified  
F1: 2 * (Precision * Recall) / (Precision + Recall) Harmonic Mean of Precision and Recall  

In this case, the algorithm does not exactly perform to expectations with an F1 score below 0.5. Using the 10th season 
of data (23/24) for unrelegated teams the following results are observed. 
![image]({{ '/assets/img/Data_Folder/2023-09-23-footy/result2.png' | relative_url }})

The predictions are relatively consistent with the previous results, we use the following parameter grid and perform a 
GridSearch on the hyperparameters. Given the small training data and a relatively small parameter space, I do not opt to use
RandomSearchCV. 

```python
param_grid = {
    'max_depth': [3, 4, 5],
    'learning_rate': [0.1, 0.01, 0.05],
    'gamma': [0, 0.25, 1.0],
    'reg_lambda': [0, 1.0, 10.0],
    'scale_pos_weight': [1, 3, 5]
}

optimal_params = GridSearchCV(
    estimator=xgb.XGBClassifier(objective='binary:logistic',
                                seed=42,
                                subsample=0.9,
                                colsample_bytree=0.5),
    param_grid=param_grid,
    scoring='roc_auc',  
    verbose=2,
    n_jobs=10,
    cv=3
)
```

With the optimised parameters we arrive at the following results. 
![image]({{ '/assets/img/Data_Folder/2023-09-23-footy/result3.png' | relative_url }})

|          | Precision | Recall   | F1       |
|:---------|:----------|:---------|----------|
| Away Win | 0.470588  | 0.444444 | 0.457143 |
| Draw     | 0.113208  | 0.333333 | 0.169014 |
| Home Win | 0.763158  | 0.537037 | 0.630435 |

From the F1 score, we see that the model only improves at predicting what it is already good at predicting which is the 
Home Win statistic. Intuitively this makes sense because most of the time, home teams in football win games because of 
the advantage of playing at home. Some clubs even make it uncomfortable for opponents to travel and settle down at their 
stadium which disrupts the rhythm of the game. Intangible factors like these are hard to quantify and often heavily influence
the results of football games. 

Data Selection 2
--------------------------
The features (betting statistics) used are in terms of odds. Odds are not normally distributed , therefore
I will be taking the natural log of the odds to ensure that data used for training comes from a normal distribution. 

My hypothesis is that if the odds are taken from a normal distribution, finding thresholds to split the data would be 
easier. Since the subset of data is small, XGBoost would use the Greedy Algorithm to find the best split and having the 
thresholds normally distributed would bring the feature data "closer together" and more "uniform".

The same features are used previously, and unfortunately we observe the same confusion matrix. We observe the following
result when the model is trained on probability. 

It seems like the above data transformation techniques do not work at all and more analysis and feature selection and 
the range of data taken is likely to be the problem. One of the key reasons why normalization and standardization might 
not provide significant benefits for XGBoost is that tree-based models, including XGBoost, are less sensitive to feature 
scale variability compared to linear models. Decision trees split nodes based on feature values, so changing the scale 
of a feature does not necessarily impact the tree structure or the model's performance. In fact, for many problems, it's 
not necessary to normalize or standardize features when using XGBoost.












