---
layout: post
title: Kernel Density Estimation
subtitle: Non parametric kernel density estimation
tags: [test, statistics, probability-distributions]
comments: true
js:
    - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML
---

While dealing with real world data, the data does not often follow a friendly statistical distribution. 
Don't let the fancy name scare you off – it's actually a super handy in visualising the probability distribution function 
and allow you to hypothesise what distribution the data follows.

Picture this: you've got a bunch of data points, and you want to get a sense of how they're spread out. 
Maybe it's the heights of your friends, the scores on your latest video game, or the amount of time you waste on social 
media every day. Whatever it is, you want to peek into the data and see those hidden patterns.

Here's the scoop on how KDE works: it places these little "kernel" functions at each data point. These kernels are like 
mini curves that represent the data's distribution around that point. Imagine you've got a bunch of hills on a landscape
– each hill is centered on a data point, and they all add up to create this smooth, rolling terrain that reflects the 
overall data distribution. 

What's really cool is that KDE lets you choose how wide or narrow those kernel curves should be. This is called the 
"bandwidth," and it's like adjusting the volume on your favorite playlist – you can make it super loud and bumpy, or 
smooth and mellow. A wider bandwidth means a smoother curve that might gloss over small bumps in your data, while a 
narrow one picks up every little wiggle.

Kernel density estimation is like the artist of the data science world. It takes your raw data and turns it into a 
masterpiece of smooth curves, helping you spot those hidden insights with ease. 


#### For a more in depth explanation:

An example of a kernel commonly used is the Gaussian kernel, given by the formula:

$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
$$

where $$\mu$$ is the mean and $$\sigma$$ is the standard deviation.

Each of these gaussian curves are initialised at every data point as shown below. Their superposition creates the over 
kernel density estimation which gives a rough approximation of the probability density function. 

For more intuition into bandwidth, the actual kernel function is given by: 

$$
K(x) = \frac{1}{h\sqrt{2\pi}}\exp{-\frac{1}{2}(\frac{x-x_i}{h})^2}
$$

where $$h$$ is the bandwidth parameter. From the two equations given, one can note that the bandwidth parameter is 
akin to the standard deviation. Intuitively speaking, a larger bandwidth parameter would result in a larger standard 
deviation for each Gaussian kernel. It follows that the superposition of each of these kernels would result in a 
smoother probability distribution function because more neighbouring data points are included in the summation. 
Thus increasing the overall bias and decreasing the overall variance of the fit. Conversely, decreasing the bandwidth 
parameter decreases the standard deviation and the contribution to the overall shape is smaller, resulting in a more 
"spiky" looking probability distribution function. 

In modern Machine Learning sense, the bandwidth is a hyperparmeter which must be tuned to find the perfect bias-variance
trade off. There are a few plug-in and rule of thumb algorithms to find the best bandwidth selection based on your needs.
The most popular one out there is the [Sheather Jones](https://www.jstor.org/stable/2345597) algorithm, which aims to minimise the mean integrated squared error. 

There are other algorithms like the [maximum likelihood cross validation](https://www.sciencedirect.com/science/article/abs/pii/0010480985900060)
which seeks to maximise the likelihood of the data coming from that specific distribution with that respective bandwidth parameter. This is akin to maximum 
likelihood estimation performed for finding the best parameters for parametric distributions. 

![image]({{ '/assets/img/Data_Folder/2023-08-12-kde/kde_plot.png' | relative_url }})

![image2](https://en.wikipedia.org/wiki/File:Comparison_of_1D_histogram_and_KDE.png)