---
layout: post
title: Decision Trees, Bagging and Boosting
subtitle: The basics of decision trees, bagging and boosting
tags: [Machine Learning, Data Science, Python, Decision Trees, Football, Premier League]
comments: true
js:
    - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML
---

Decision Trees are a great machine learning algorithm to use, especially for hackathons. They can be used for regression 
and classification tasks. Ensemble methods are often combined with decision trees to produce better machine learning models, 
less prone to overfitting. Ensemble methods mean combining multiple machine learning models to produce one final model. 
The most common ensemble methods for decision trees are boosting and bagging. 

Each algorithm has their own merits. Bootstrap aggregation (bagging) generally involves creating multiple models which are
equally weighted and combined to produce a final prediction, since each model has equal say, bagging is effective in decreasing
the variance because an aggregation of votes is used. Boosting involves creating multiple models which are weighted according to their performance
and combined to produce a final prediction. Since each model is weighted according to its performance, boosting is more effective
in decreasing the bias. 

Boosting combines the outputs of many "weak" classifiers to produce a powerful "committee". The weak classifiers
can be thought of as simple decision trees that are only slightly better than random guessing. The committee is formed
by training many of these weak classifiers on different subsets of the data. The final prediction is then made by
combining the outputs of all the weak classifiers.

Gradient Boost
* Start with average value of variable you want to predict / Start with the log odds of the probability of the event you want to predict
* Compute the residuals which is (Observed - Predicted)
* Fit a decision tree to the residuals 
* Scale the tree by a small number (learning rate) and add it to the previous tree. A small learning rate reduces the 
effect of each tree and allows for more trees to be added to the model, improving the accuracy in the long run. 
* Repeat steps 2-4 until the residuals are no longer improving, or you reach the maximum number of trees

Note about odds and log odds 
The odds of an event is the probability of the event divided by the probability of not the event. $$Odds = \frac{P}{1 - P}$$
Log odds is the natural log of the odds, which is used to skew the odds to be more symmetrical. 
The distribution of the log of the odds follows a normal distribution. For the gradient boost classification algorithm, 
the log odds of the probability of the event is used as the initial prediction, the output value of each leaf is also
in terms of log odds. The final value for the odds which has been scaled by the learning rate is converted back to a 
value for probability. 

AdaBoost vs Gradient Boost
--------------------------
ADA Boost builds a stump (tree with one node and two leaves), and the amount of say the stump has on the final output is determined by how well it performs
on the training set. If it performs well, it has a high say, if it performs poorly, it has a low say. AdaBoost then builds another stump
based on the errors of the first stump and repeats the process.

Gradient boost starts by making an initial guess, the tree is usually larger than the stump, but gradient boost restricts the 
size of the tree. Gradient boost builds a tree to fit the errors of the previous tree. Gradient boost scales all the trees
by the same amount but AdaBoost scales the trees based on how well they perform on the training set.

The main algorithm that interests me the most is the XGBoost, which makes use of gradient boost but also prunes the trees to 
reduce variance.

XGBoost for Classification
--------------------------
* Initial prediction of which class the observation belongs to e.g. 50% 
* Fit a decision tree to the residuals

A calculation of similarity scores and gain are used to decide how to split the data:

Similarity score: $$\frac{(\sum Residual_i)^2}{\sum [Previous\:Probability_i \times (1 - Previous\:Probability_i)] + \lambda}$$
Intuition: 
Gain: Left similarity score + Right similarity score - Root similarity score
Pruning condition: Gain - Gamma (Tree Complexity Parameter)


The tree starts of as one leaf, all the observations go into that one leaf, and the residuals are calculated. The residuals
are calculated. Recalculate similiarity scores for the leaf by splitting using a feature.

The similarity scores for the leaf nodes are added together to find the gain. The gain is compared for different splits
and the split with the highest gain is chosen. One can limit the number of levels, and a threshold for the minimum number of 
residuals in each leaf. 

The minimum number of residuals in leaf is found by cover. Cover has different formulas for regression and classification. 
The trees are pruned using Gamma

Cover depends on previously predicted probability for each residual in a life as given by the formula above. 
min_child_weight parameter: minimum value is 0 if not XGBoost would not allow leaf nodes. 

Gamma: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.
Lambda: L2 regularization term on weights. Increasing this value will make model more conservative, lower values of gain. 

Values of lambda greater than 0 reduce the sensitivity of the tree to individual observations by pruning and combining them
with other observations 



