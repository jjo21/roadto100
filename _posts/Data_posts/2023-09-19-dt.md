---
layout: post
title: Decision Trees Algorithm for Football prediction
subtitle: Feature Selection, Data Cleaning, Model Training Prediction and Evaluation
tags: [Machine Learning, Data Science, Python, Decision Trees, Football, Premier League]
comments: true
js:
    - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML
---

Decision Trees are a great machine learning algorithm to use, especially for datathons. They can be used for regression 
and classification tasks.

Ensemble methods are often combined with decision trees to produce better machine learning models, less prone to overfitting.
Ensemble methods mean combining multiple machine learning models to produce one final model. The most common ensemble methods
for decision trees are boosting and bagging. 

In regular decision trees 

* Boosting: It combines the outputs of many "weak" classifiers to produce a powerful "committee". The weak classifiers
can be thought of as simple decision trees that are only slightly better than random guessing. The committee is formed
by training many of these weak classifiers on different subsets of the data. The final prediction is then made by
combining the outputs of all the weak classifiers. The most common way to combine the outputs is by taking a majority
vote. Boosting is a general technique that can be applied to any machine learning algorithm. It is most commonly used
with decision trees.

Gradient Boost (Regression)
1) Start with average value of variable you want to predict
2) Compute the residuals which is (Observed - Predicted)
3) Fit a decision tree to the residuals
4) Scale the tree by a small number (learning rate) and add it to the previous tree. A small learning rate reduces the 
effect of each tree and allows for more trees to be added to the model, improving the accuracy in the long run. 
5) Repeat steps 2-4 until the residuals are no longer improving, or you reach the maximum number of trees

Gradient Boost (Classification)
1) Start with the log odds of the probability of the event you want to predict
2) Compute the residuals which is (Observed - Predicted). Predictions are in terms of log(odds), and since the leaves are derived 
from probabilities, a transformation is required to find the output values later. 
3) Fit a decision tree to the residuals
4) Compute the output values of the tree and scale it by the learning rate
5) Build another tree based on new residuals and recalculate output values
6) Repeat until residuals are tiny, or you reach the maximum number of trees

Important Segway about odds and log odds 
The odds of an event is the probability of the event divided by the probability of not the event.
Odds = P(event)/P(not event) 
Log odds is the natural log of the odds, which is used to skew the odds to be more symmetrical. The distribution
of the log of the odds follows a normal distribution.

ADA Boost vs Gradient Boost 

ADA Boost builds a stump, and the amount of say the stump has on the final output is determined by how well it performs
on the training set. If it performs well, it has a high say, if it performs poorly, it has a low say. AdaBoost then builds another stump
based on the errors of the first stump

Gradient boost starts by making an initial guess, the tree is usually larger than the stump, but gradient boost restricts the 
size of the tree. Gradient boost builds a tree to fit the errors of the previous tree. Gradient boost scales all the trees
by the same amount but AdaBoost scales the trees based on how well they perform on the training set.

XGBoost (Classification)
1) Initial prediction of which class the observation belongs to 
2) Fit an XGBoost tree to the residuals
3) When XGBoost is used for regression 
3) Find the similarity score between the observations in each leaf of the tree and the observations in the training set

