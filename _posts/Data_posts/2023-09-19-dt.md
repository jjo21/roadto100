---
layout: post
title: Decision Trees Algorithm for Football prediction
subtitle: Feature Selection, Data Cleaning, Model Training Prediction and Evaluation
tags: [Machine Learning, Data Science, Python, Decision Trees, Football, Premier League]
comments: true
js:
    - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML
---

Decision Trees are a great machine learning algorithm to use, especially for datathons. They can be used for regression 
and classification tasks. Ensemble methods are often combined with decision trees to produce better machine learning models, 
less prone to overfitting. Ensemble methods mean combining multiple machine learning models to produce one final model. 
The most common ensemble methods for decision trees are boosting and bagging. 

* Boosting: It combines the outputs of many "weak" classifiers to produce a powerful "committee". The weak classifiers
can be thought of as simple decision trees that are only slightly better than random guessing. The committee is formed
by training many of these weak classifiers on different subsets of the data. The final prediction is then made by
combining the outputs of all the weak classifiers. The most common way to combine the outputs is by taking a majority
vote. Boosting is a general technique that can be applied to any machine learning algorithm. It is most commonly used
with decision trees.

Gradient Boost (Regression)
* Start with average value of variable you want to predict 
* Compute the residuals which is (Observed - Predicted)
* Fit a decision tree to the residuals 
* Scale the tree by a small number (learning rate) and add it to the previous tree. A small learning rate reduces the 
effect of each tree and allows for more trees to be added to the model, improving the accuracy in the long run. 
* Repeat steps 2-4 until the residuals are no longer improving, or you reach the maximum number of trees

Gradient Boost (Classification)
* Start with the log odds of the probability of the event you want to predict
* Compute the residuals which is (Observed - Predicted). Predictions are in terms of log(odds), and since the leaves are derived 
from probabilities, a transformation is required to find the output values later. 
* Fit a decision tree to the residuals
* Compute the output values of the tree and scale it by the learning rate
* Build another tree based on new residuals and recalculate output values
* Repeat until residuals are tiny, or you reach the maximum number of trees

Important Segway about odds and log odds 
The odds of an event is the probability of the event divided by the probability of not the event.
Odds = P(event)/P(not event) 
Log odds is the natural log of the odds, which is used to skew the odds to be more symmetrical. The distribution
of the log of the odds follows a normal distribution.

ADA Boost vs Gradient Boost 

ADA Boost builds a stump, and the amount of say the stump has on the final output is determined by how well it performs
on the training set. If it performs well, it has a high say, if it performs poorly, it has a low say. AdaBoost then builds another stump
based on the errors of the first stump

Gradient boost starts by making an initial guess, the tree is usually larger than the stump, but gradient boost restricts the 
size of the tree. Gradient boost builds a tree to fit the errors of the previous tree. Gradient boost scales all the trees
by the same amount but AdaBoost scales the trees based on how well they perform on the training set.

The main algorithm that interests me the most is the XGBoost. 

XGBoost (Classification) 
* Initial prediction of which class the observation belongs to 
* Fit a decision tree to the residuals

Formula for similarity squares: 

$$\frac{(\sum Residual_i)^2}{\sum [Previous\:Probability_i \times (1 - Previous\:Probability_i)] + \lambda}$$

The tree starts of as one leaf, all the observations go into that one leaf, and the residuals are calculated. The residuals
are calculated. Recalculate similiarity scores for the leaf by splitting using a feature.

The similarity scores for the leaf nodes are added together to find the gain. The gain is compared for different splits
and the split with the highest gain is chosen. One can limit the number of levels, and a threshold for the minimum number of 
residuals in each leaf. 

The minimum number of residuals in leaf is found by cover. Cover has different formulas for regression and classification. 
The trees are pruned using Gamma

Cover depends on previously predicted probability for each residual in a life as given by the formula above. 
min_child_weight parameter: minimum value is 0 if not XGBoost would not allow leaf nodes. 

Gamma: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.
Lambda: L2 regularization term on weights. Increasing this value will make model more conservative, lower values of gain. 

Values of lambda greater than 0 reduce the sensitivity of the tree to individual observations by pruning and combining them
with other observations 



