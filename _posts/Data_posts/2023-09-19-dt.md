---
layout: post
title: Decision Trees, Bagging and Boosting
subtitle: The basics of decision trees, bagging and boosting
tags: [Machine Learning, Data Science, Python, Decision Trees]
comments: true
js:
    - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML
---

Decision Trees are a great machine learning algorithm to use, especially for hackathons. They can be used for regression 
and classification tasks. Ensemble methods are often combined with decision trees to produce better machine learning models, 
less prone to overfitting. Ensemble methods mean combining multiple machine learning models to produce one final model. 
The most common ensemble methods for decision trees are boosting and bagging. 

Each algorithm has their own merits. Bootstrap aggregation (bagging) generally involves creating multiple models which are
equally weighted and combined to produce a final prediction, since each model has equal say, bagging is effective in decreasing
the variance because an aggregation of votes is used. Boosting involves creating multiple models which are weighted according to their performance
and combined to produce a final prediction. Since each model is weighted according to its performance, boosting is more effective
in decreasing the bias. 

Boosting combines the outputs of many "weak" classifiers to produce a powerful "committee". The weak classifiers
can be thought of as simple decision trees that are only slightly better than random guessing. The committee is formed
by training many of these weak classifiers on different subsets of the data. The final prediction is then made by
combining the outputs of all the weak classifiers. 

As an example here is a brief overview of the Gradient Boosting algorithm:
* Start with average value of variable you want to predict / Start with the log odds of the probability of the event you want to predict
* Compute the residuals which is (Observed - Predicted)
* Fit a decision tree to the residuals 
* Scale the tree by a small number (learning rate) and add it to the previous tree. A small learning rate reduces the 
effect of each tree and allows for more trees to be added to the model, improving the accuracy in the long run. 
* Repeat steps 2-4 until the residuals are no longer improving, or you reach the maximum number of trees

__Note about odds and log odds:__
The odds of an event is the probability of the event divided by the probability of not the event: $$Odds = \frac{P}{1 - P}$$.
Log odds is the natural log of the odds, which is used to skew the odds to be more symmetrical. 
The distribution of the log of the odds follows a normal distribution. For the gradient boost classification algorithm, 
the log odds of the probability of the event is used as the initial prediction, the output value of each leaf is also
in terms of log odds. The final value for the odds which has been scaled by the learning rate is converted back to a 
value for probability. 

AdaBoost vs Gradient Boost
--------------------------
ADA Boost builds a stump (tree with one node and two leaves), and the amount of say the stump has on the final output is determined by how well it performs
on the training set. If it performs well, it has a high say, if it performs poorly, it has a low say. AdaBoost then builds another stump
based on the errors of the first stump and repeats the process.

Gradient boost starts by making an initial guess, the tree is usually larger than the stump, but gradient boost restricts the 
size of the tree. Gradient boost builds a tree to fit the errors of the previous tree. Gradient boost scales all the trees
by the same amount but AdaBoost scales the trees based on how well they perform on the training set.

The main algorithm that interests me the most is the XGBoost, which makes use of gradient boost but also prunes the trees to 
reduce variance.

XGBoost for Classification
--------------------------
* Initial prediction of which class the observation belongs to e.g. 50% 
* Fit a decision tree to the residuals

A calculation of similarity scores and gain are used to decide how to split the data:

{: .box-note}
**Similarity score:** $$\frac{(\sum Residual_i)^2}{\sum [Previous\:Probability_i \times (1 - Previous\:Probability_i)] + \lambda}$$  
_Intuition_: Assess the quality of a potential split at a node in a decision tree. The higher the similarity score,
the better the split.

{: .box-note}
**Gain:** Left similarity score + Right similarity score - Root similarity score  
_Intuition_: Asses the quality of a split by comparing the similarity scores of the left and right nodes to the root node.
The higher the gain, the better the improvement in prediction accuracy. 

{: .box-note}
**Pruning condition:** Gain - Gamma (Tree Complexity Parameter)  
_Intuition_: The higher the Gain - Gamma, then the split is satisfactory and the tree is not pruned. 

{: .box-note}
**Lambda:** Lambda is used in both the similarity score formula and output value formula for each tree. The output value
formula is given by $$\frac{\sum Residual_i}{\sum [Previous\:Probability_i \times (1 - Previous\:Probability_i)] + \lambda}$$.  
_Intuition_: If the value of lambda is large, the demoninator is larger hence returning a smaller output value and similarity score.
This reduces the size of the gain and hence the tree is more likely to be pruned.

{: .box-note}
**Cover:** $$\frac{\sum Previous\:Probability_i \times (1 - Previous\:Probability_i)}{\sum Residual_i}$$. Also known as 
the denominator of the similarity score (for classification).  
Intuition: If a node has high cover, it means a significant amount of training data falls into that node. The decisions 
made at the node are based on a substantial amount of information, the prediction is likely to be more robust. 

{: .box-note}
**Learning Rate:** The learning rate scales the final output value of the tree before another tree is created to predict 
the residuals.  
Intuition: A small learning rate reduces the effect of each tree acting as a form of regularization and helps prevent 
overfitting. 

Gain, Gamma, Lambda, Cover and Learning Rate are some of the hyperparmaters used in building the decision tree to find the output values. 

The tree starts off as one leaf and all the observations go into that one leaf, the residuals are then calculated. The output 
values are used to build the next tree and the tree keeps growing until the residuals are no longer improving or the 
maximum number of trees has been reached.

This brings me to the end of a brief summary of bagging and boosting algorithms, I hope you found it useful and hopefully
next time I have more time to go through some of the math and other things you can do with it. 





